{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    The class that is used to create the mapping of \"token to index\" and \"index to toekn\".\n",
    "    It implements the following functions:\n",
    "    \n",
    "    - add_token:\n",
    "        add one specified token into the mapping (self._token_to_idx and self._idx_to_token)\n",
    "    - add_many:\n",
    "        add a list of tokens, return indices(list)\n",
    "    - lookup_token:\n",
    "        return the index of the token, if the token is in the vocabulary\n",
    "    - lookup_idx:\n",
    "        return the token of the index, if the index is in the self._idx_to_token\n",
    "    -__len__:\n",
    "        return the size of the vocabulary\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,token_to_idx = None,add_unk = True,unk_token = '<unk>'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        token_to_idx (dict), the existing dictionary mapping token into idx\n",
    "        add_unk(bool), add unknow token into the vocabulary or not\n",
    "        unk_token(str), the unkown token   \n",
    "        \"\"\" \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for idx,token in self._token_to_idx.items()}\n",
    "        \n",
    "        # Add unk_token into the vocabulary\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._unk_idx = -1\n",
    "        \n",
    "        if self._add_unk:\n",
    "            self._unk_idx = self.add_token(self._unk_token)\n",
    "            \n",
    "    def add_token(self,token):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token: The token that is going to be added\n",
    "        Output:\n",
    "            the index of the token\n",
    "        \"\"\"\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            \n",
    "        return index\n",
    "    \n",
    "    def add_many(self,tokens):\n",
    "        \"\"\"\n",
    "        Add the list of tokens into the vocabulary\n",
    "        Args:\n",
    "            tokens(str),a list of tokens that are going to be added into the vocabulary\n",
    "        Output:\n",
    "            indices(list), the list of indices corresponding to tokens\n",
    "        \"\"\"\n",
    "        indices = [self._idx_to_token[token] for token in tokens]\n",
    "        return indices\n",
    "    \n",
    "    def lookup_token(self,token):\n",
    "        \"\"\"\n",
    "        Retrieve the index of the token, if the token does not exist,then return the unkown token\n",
    "        \n",
    "        Output:\n",
    "            the index of the token\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._unk_idx > 0:\n",
    "            return self._token_to_idx.get(token,self._unk_idx)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    def look_index(self,index):\n",
    "        \"\"\"\n",
    "        Return the index of the token. if the index does not exist in the vocabulary, then \n",
    "        raise the error\n",
    "        \n",
    "        Args:\n",
    "            the wanted index of the token\n",
    "        Output:\n",
    "            the assoicated index\n",
    "        \n",
    "        \"\"\"\n",
    "        if index in self._index_to_token:\n",
    "            return self._index_to_token[index]\n",
    "        else:\n",
    "            raise KeyError(\"the index (%d) is not in the vocabulary\", index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\"\n",
    "    This class is to vectorize the input text into vectors\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, review_vocab, label_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab ( the instance of Vocabulary): mapping tokens into integers\n",
    "            label_vocab (the instance of Vocabulary): mapping the label into integers\n",
    "        \n",
    "        \"\"\"\n",
    "        self.review_vocal = review_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls,review_data,cutoff = 20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_data (pandas.DataFrame) : the dataset\n",
    "            cutoff(int): only tokens that have higher frequencies can be added into the Vocabulary \n",
    "        Output:\n",
    "            the instance of Vectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        label_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        word_counter = Counter()\n",
    "        for review in review_data.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counter[word] += 1\n",
    "                    \n",
    "        for word,count in word_counter.items():\n",
    "            if count >= cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        \n",
    "        return cls(review_vocab,label_vocab)\n",
    "    \n",
    "    def vectorizeOnehot(self,review):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review(str), the text\n",
    "        Output:\n",
    "            the one-hot vector representing the text\n",
    "        \"\"\"\n",
    "        \n",
    "        one_hot = np.zeros(len(self.review_vocab))\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Instantiate a Vectorizer from a serializable dictionary\n",
    "        \n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the Vectorizer class\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        label_vocab =  Vocabulary.from_serializable(contents['label_vocab'])\n",
    "\n",
    "        return cls(review_vocab=review_vocab, label_vocab=label_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        \n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'label_vocab': self.label_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDateset(Dataset):\n",
    "    def __init__(self,review_df,vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df(dataFrame), the review text\n",
    "            vectorizer: the instance of Vectorizer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "                            \"train\": (self.train_df,self.train_size),\n",
    "                            \"test\": (self.test_df,self.test_size),\n",
    "                            \"val\": (self.val_df,self.val_size)\n",
    "                            }\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_csv(str): location of the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split == \"train\"]\n",
    "        \n",
    "        return cls(review_df, Vectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "        \n",
    "    def set_split(self,split = \"train\"):\n",
    "        \"\"\"\n",
    "        Select the data whose split attribute is \"split\"\n",
    "        Args:\n",
    "            split(string), one of \"train\",\"test\",\"val\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of the data\n",
    "        Return:\n",
    "            the dictionary of features(X) and the label(Y)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vec = self._vectorizer.vectorizeOnehot(row.review)\n",
    "        \n",
    "        label_index = self._vectorizer.label_vocab.lookup_token(row.label)\n",
    "        \n",
    "        return {\"X_data\":review_vec,\n",
    "                \"Y_label\": label_index}\n",
    "    \n",
    "    def get_num_batches(self,batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size(int), given a batch size, return how many batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "        \n",
    "def generate_batches(dataset,batch_size, device = 'cpu',\n",
    "                     shuffle = True, drop_last = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a generator that wraps the DataLoader. It will ensure the tensors are one the right device\n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(dataset = dataset, batch_size= batch_size, shuffle=shuffle,\n",
    "                            drop_last = drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,)\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed and make path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/vectorizer.json\n",
      "\tmodel_storage/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv= 'D:/Study/Jupyter/nlp begineer/task1/processed_data.csv',\n",
    "    # review_csv='data/yelp/reviews_with_splits_full.csv',\n",
    "    save_dir='model_storage/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No Model hyper parameters\n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Phrase', 'Sentiment', 'split'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tem_data = pd.read_csv(args.review_csv)\n",
    "print(tem_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReviewDateset' object has no attribute 'save_vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-625136c0c6cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReviewDateset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset_and_make_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReviewClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ReviewDateset' object has no attribute 'save_vectorizer'"
     ]
    }
   ],
   "source": [
    "dataset = ReviewDateset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv(args.raw_train_csv,sep = '\\t')\n",
    "raw_test = pd.read_csv(args.raw_test_csv,sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060 66292\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_train),len(raw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{7072, 9206, 27273, 32927, 79582}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_train.columns)\n",
    "set(raw_train.Sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the subset by sentiments to create new_train and val splits\n",
    "This step is to create new_train, val sets to ensure the same distribution of sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentiment', 'Phrase'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "raw_train_data = raw_train[[\"Sentiment\",\"Phrase\"]]\n",
    "raw_test_data = raw_test[[\"Phrase\"]]\n",
    "print(raw_train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sentiment = collections.defaultdict(list)\n",
    "for _,row in raw_train_data.iterrows():\n",
    "    by_sentiment[row.Sentiment].append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Sentiment': 0, 'Phrase': 'would have a hard time sitting through this one'}, {'Sentiment': 0, 'Phrase': 'have a hard time sitting through this one'}, {'Sentiment': 0, 'Phrase': 'Aggressive self-glorification and a manipulative whitewash'}]\n"
     ]
    }
   ],
   "source": [
    "print(by_sentiment[0][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create split data\n",
    "a = {2:[{'rat':1,'bcd':2}],1:[{'rat':3,'bcd':4}]} the form of by_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = Namespace(\n",
    "    raw_train_csv = 'D:/Study/Jupyter/nlp begineer/task1/train.tsv',\n",
    "    raw_test_csv = 'D:/Study/Jupyter/nlp begineer/task1/test.tsv',\n",
    "    train_proportion = 0.7 ,\n",
    "    val_proportion = 0.15,\n",
    "    test_proportion = 0.15,\n",
    "    output_csv = 'D:/Study/Jupyter/nlp begineer/task1/processed_data.csv',\n",
    "    seed = 131\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _,item_list in by_sentiment.items():\n",
    "    #item_list = [{'Sentiment: n, Phrase: xxx'},{'Sentiment: n, Phrase: xxx'},..] Each dict is a data\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total)\n",
    "    n_val = int(args.val_proportion * n_total)\n",
    "    n_test = int(args.test_proportion * n_total)\n",
    "    \n",
    "    # Give the corresponding attribute to data\n",
    "    for item in item_list[:n_train]:\n",
    "        # item = {'Sentiment: n, Phrase: xxx'} is a dict, assign a attribute\n",
    "        item['split'] = 'train'\n",
    "    \n",
    "    for item in item_list[n_train:n_train + n_test]:\n",
    "        item['split'] = 'test'\n",
    "    \n",
    "    for item in item_list[n_train + n_test:]:\n",
    "        item['split'] = 'val'\n",
    "    \n",
    "    fin_list.extend(item_list)\n",
    "fin_data = pd.DataFrame(fin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # lowercase the text\n",
    "    text =text.lower()\n",
    "    text = re.sub(r'([.,!?])',r' \\1 ',text)\n",
    "    text = re.sub(r'[^a-z.,!?]+',r' ',text)\n",
    "    return text\n",
    "\n",
    "fin_data.Phrase = fin_data.Phrase.apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data.to_csv(data_args.output_csv,index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_data = pd.read_csv(data_args.output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_data = tem_data.rename(columns= {'Phrase':'review','Sentiment':'label','Split':'split'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'label', 'Split'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tem_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_data.to_csv(data_args.output_csv,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
